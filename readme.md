# TEI Deployment with Qdrant Cache and Nginx Load Balancer

This repository provides a configuration to serve a Hugging Face text embedding model using `text-embeddings-inference` (TEI) deployed across multiple GPUs. It includes a caching layer built with FastAPI and Qdrant to optimize performance for repeated requests, and uses Nginx internally to load balance requests to the TEI instances for cache misses.

## Architecture

1.  **Client:** Sends embedding requests to the Cache Proxy service.
2.  **Cache Proxy (FastAPI Service):**
    *   Acts as the main entry point for all embedding requests.
    *   Receives requests, calculates hashes (e.g., SHA256) of the input texts.
    *   Queries the Qdrant database using these hashes as Point IDs to check for cached embeddings.
    *   **Cache Hit:** Retrieves the embedding vector(s) directly from Qdrant and returns them to the client.
    *   **Cache Miss:** Forwards *only the texts not found in the cache* to the internal Nginx load balancer.
    *   Receives the computed embeddings for the missed texts from Nginx.
    *   Stores these new text/hash/embedding triplets in Qdrant for future requests.
    *   Combines cached and newly computed results and returns the complete list of embeddings to the client.
3.  **Qdrant (Vector Database):**
    *   Acts as a persistent, shared cache.
    *   Stores embeddings keyed by the hash of the input text.
    *   Data persists across container restarts thanks to a Docker volume.
4.  **Nginx (Internal Load Balancer):**
    *   Receives requests *only* for cache misses from the Cache Proxy service.
    *   Load balances these requests across the available TEI instances using a round-robin (or other configured) strategy.
    *   Is not exposed externally; communication happens within the Docker network.
5.  **TEI Instances (Text Embeddings Inference):**
    *   Multiple instances (one per GPU, configured via `NUM_REPLICAS`).
    *   Receive requests from Nginx for texts not found in the cache.
    *   Compute the text embeddings using the specified Hugging Face model on their assigned GPU.
    *   Return the computed embeddings back through Nginx to the Cache Proxy.

This setup aims to significantly speed up responses for frequently requested texts while distributing the computational load for new texts across multiple GPUs.

## Directory Structure

```
tei-nginx-qdrant-cache/
├── .env                  # Environment variables (REQUIRED)
├── .gitignore
├── docker-compose.yml    # Generated by script
├── generate_configs.py   # Python script to generate configs
├── nginx/                # Nginx configuration directory
│   └── nginx.conf      # Generated by script
├── cache_proxy/          # FastAPI Cache Proxy service code
│   ├── Dockerfile
│   ├── main.py
│   ├── requirements.txt
│   ├── config.py
│   ├── schemas.py
│   └── qdrant_utils.py
└── README.md             # This file
```

## Prerequisites

*   Python 3.8+ (for the generation script)
*   Docker Engine
*   Docker Compose (v2 or later recommended: `docker compose` command)
*   NVIDIA GPU Drivers (matching your hardware)
*   NVIDIA Container Toolkit (to allow Docker containers access to GPUs)
*   A multi-GPU machine (or adjust `NUM_REPLICAS` for single GPU testing)

## Configuration

1.  **Clone the repository:**
    ```bash
    git clone <your-repo-url>
    cd tei-nginx-qdrant-cache
    ```
2.  **Prepare the Cache Proxy Code:** Ensure the `cache_proxy/` directory exists and contains all the necessary Python files (`main.py`, `config.py`, `schemas.py`, `qdrant_utils.py`), `requirements.txt`, and the `Dockerfile`.

3.  **Create/Edit the `.env` file:**
    This file is crucial for configuring the entire stack. Create a `.env` file in the root directory with the following variables:

    ```dotenv
    # .env Example

    # --- Model Configuration ---
    MODEL_ID=BAAI/bge-large-en-v1.5 # IMPORTANT: Choose a TEI-compatible model
    EMBEDDING_DIMENSION=1024        # !!! CRITICAL: Set to your MODEL_ID's output dimension !!!
    # REVISION=main                 # Optional: Specify a model revision (branch, tag, commit)

    # --- Deployment Configuration ---
    NUM_REPLICAS=8                  # Number of TEI instances (match GPU count)
    HOST_PORT=8080                  # External port for the CACHE PROXY service

    # --- TEI Container Configuration ---
    TEI_IMAGE=ghcr.io/huggingface/text-embeddings-inference:latest # Or pin version e.g., :1.6
    MAX_BATCH_TOKENS=16384          # Adjust based on model and GPU memory
    # DTYPE=float16                 # Optional: Hardware float precision (e.g., float16, bfloat16)
    # HUGGING_FACE_HUB_TOKEN=hf_... # Optional: For private models

    # --- Qdrant Cache Configuration ---
    QDRANT_COLLECTION=text_embedding_cache # Name for the Qdrant collection
    QDRANT_HOST=qdrant                     # Qdrant service name within Docker network
    QDRANT_PORT=6333                       # Qdrant gRPC port (internal)

    # --- Cache Proxy Configuration ---
    # URL for the Nginx load balancer *inside* the docker network
    NGINX_UPSTREAM_URL=http://nginx-internal-lb:80 # Use the container name of nginx service
    CACHE_HASH_FUNCTION=sha256             # Hash algorithm (sha256 recommended, md5 faster but riskier)
    ```
    *   **Set `MODEL_ID`** to a model compatible with Text Embeddings Inference.
    *   **Set `EMBEDDING_DIMENSION`** accurately based on the chosen `MODEL_ID`. This is essential for Qdrant.
    *   Adjust `NUM_REPLICAS` to match the number of GPUs you want to use.
    *   Verify `NGINX_UPSTREAM_URL` matches the `container_name` given to the Nginx service in the generated `docker-compose.yml` (it's `nginx-internal-lb` in the current script).

4.  **Generate Configuration Files:**
    Run the Python script. It reads `.env` and creates/overwrites `docker-compose.yml` and `nginx/nginx.conf`.
    ```bash
    python generate_configs.py
    ```
    Verify the output indicates successful generation. You can inspect the generated files to ensure they reflect your `.env` settings.

## Usage

1.  **Build and Start the Services:**
    *(Ensure you have run `python generate_configs.py` first)*
    Use `docker compose up`. The `--build` flag is necessary the first time or whenever you change the code in the `cache_proxy` directory. The `-d` flag runs the containers in detached mode (in the background).
    ```bash
    docker compose up -d --build
    ```
    This will:
    *   Build the `cache-proxy` Docker image.
    *   Pull the Qdrant, Nginx, and TEI images.
    *   Create the Docker network and volumes.
    *   Start all containers based on the generated `docker-compose.yml`.
    The first time, TEI containers might take a while to download the specified model into the shared `tei-model-cache` volume. Qdrant will initialize its storage in the `qdrant-storage` volume.

2.  **Check Container Status:**
    ```bash
    docker compose ps
    ```
    You should see the `cache-proxy`, `qdrant-db`, `nginx-internal-lb`, and all `tei-X` containers in a running state.

3.  **Monitor Logs (Highly Recommended):**
    Tail the logs of specific services to check for errors or monitor activity.
    ```bash
    docker compose logs -f cache-proxy   # Check for cache hits/misses, Qdrant interactions, errors
    docker compose logs -f qdrant-db     # Check Qdrant startup and activity
    docker compose logs -f nginx-internal-lb # Check requests forwarded from cache-proxy
    docker compose logs -f tei-0         # Check model loading and inference logs for one TEI instance
    # Use 'docker compose logs -f' to view all logs
    ```
    Look for successful initialization messages from Qdrant and TEI, and watch the `cache-proxy` logs for request processing details.

4.  **Test the Embedding Endpoint:**
    Send POST requests to the **Cache Proxy service** on the `HOST_PORT` you configured in `.env`.

    ```bash
    # Example using curl
    curl http://localhost:8080/embed \
      -X POST \
      -d '{"inputs": "This is the first request for this text."}' \
      -H 'Content-Type: application/json'

    # Send the *exact same* input again
    curl http://localhost:8080/embed \
      -X POST \
      -d '{"inputs": "This is the first request for this text."}' \
      -H 'Content-Type: application/json'

    # Send a different input
    curl http://localhost:8080/embed \
      -X POST \
      -d '{"inputs": ["This is a new text.", "Another new one."]}' \
      -H 'Content-Type: application/json'
    ```
    *(Replace `localhost:8080` if your `HOST_PORT` is different or if accessing remotely)*

    *   **Observe:** The very first request for a specific text should trigger a cache miss (check `cache-proxy` logs) and might take slightly longer as it involves TEI inference. Subsequent requests with the *exact same text* should be significantly faster (cache hit).

5.  **Stop the Services:**
    ```bash
    docker compose down
    ```
    This stops and removes the containers but leaves the Docker volumes (`tei-model-cache`, `qdrant-storage`) intact, preserving your downloaded model and cached embeddings. To remove the volumes as well (deleting the model and cache):
    ```bash
    docker compose down -v
    ```

## Scaling / Changing GPU Count

To change the number of TEI instances:

1.  **Stop the services (if running):**
    ```bash
    docker compose down
    ```
2.  **Update `NUM_REPLICAS` in `.env`** to the new desired number of GPUs/instances.
3.  **Regenerate the configuration files:**
    ```bash
    python generate_configs.py
    ```
    This will update `docker-compose.yml` (number of `tei-X` services and dependencies) and `nginx/nginx.conf` (upstream server list).
4.  **Restart the services:**
    ```bash
    docker compose up -d # --build is likely not needed unless cache_proxy code changed
    ```
    Docker Compose will adjust the running containers to match the new configuration.

## Notes & Considerations

*   **`EMBEDDING_DIMENSION`:** Setting this correctly in `.env` is critical for Qdrant to function properly. Find the dimension from your model's documentation or Hugging Face page.
*   **Cache Invalidation:** This setup does not include explicit cache invalidation. Embeddings are cached indefinitely based on the hash of the input text. If the underlying model changes or you need to force re-computation, you would need to manually clear the Qdrant collection or implement a TTL mechanism (which would require modifying the `cache_proxy` code).
*   **Hashing:** Using SHA256 is generally safe. MD5 is faster but has a theoretically higher (though still very low for text) chance of collisions.
*   **Performance:**
    *   Cache hits should be very fast, dominated by network latency to the proxy and Qdrant lookup time.
    *   Cache misses incur the overhead of hashing, Qdrant lookup (miss), forwarding to Nginx, TEI inference, and storing the result back in Qdrant. This will be slightly slower than hitting TEI directly *without* caching.
    *   Ensure Qdrant has sufficient resources. Monitor its performance if caching becomes a bottleneck.
*   **Error Handling:** The cache proxy includes basic error handling for Qdrant and TEI communication failures. Check its logs for details if requests fail.
*   **Resource Usage:** This setup runs multiple TEI instances, Nginx, Qdrant, and the Cache Proxy. Ensure your machine has sufficient CPU, RAM, and GPU memory.
*   **Security:** The `HOST_PORT` exposes the Cache Proxy. Apply firewall rules as needed. Ensure sensitive information (like `HUGGING_FACE_HUB_TOKEN`) in `.env` is protected.
